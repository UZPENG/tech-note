# 2017年Google I/O keynote 内容
Good morning, welcome to goole I/O.
audiance:I love you Sundar.
I love you guys too.Can't believe it's one year already,it's beautiful day,We're being joined by over 7,000 peple and we are live streaming this, as always to over 400 event in 85 countries.Last year was the 10th year since Goolge I/O started and so we moved it closer to home at ShoreLine,back where it all began.Since it is gone well.I Checked the wikipedia entry from last year.There was some metions of sunburn,so we have plenty of sunscreen all around. It's on us.Use it liberally. It's been a very busy year since last year,no different from my 13 years at Google.That's because we've been focused ever more on our core mission of organizing the world's information.And we're doing it for everyone.And we approach it by applying deep computer science and technical insights to solve problems at scale.That approach has served us very well.This is what allowed us to scale up seven of our most important products and plaforms to over a billion monthly active users each.And it's not just the scale at which these products are working,users engage with them very heavily.YouTube,not just has over a billion users,but every single day,users watch over 1 billions hours of videos on Youtube.Google Maps, every single day,users navigate over 1 billion kilometers with google Maps. So the scale is inspiring to see,and our other products approaching this scale.We launched Google Drive five years ago,and today,it is over 800 million monthly active users.And every single week,there are over 3 million objects uploaded to Goolge Drive.Two years ago at Google IO,we launched Photos as a way to organize user's photo using machine learning.And todat, we are over 500 million active users,and every single day,users up load 1.2 billion photos to Google. So the scale of product is amazing,but they are all still working up to their way to what'a Android,which I'm excited as of this week,we crossed over 2 billion active devices of Android.As you can see,the robot is pretty happy too,behind me.so it's privilege to serve users of this scale.And it is all because the growth of moblie and smartphones,but computing is evolving again.We spoke last year about this impotant shift in computing from a mobile first to AI first aproach,Moblie made us reimagine every product we were working on.We had to take into account that the user interaction model had fundamentally changed,with multi-touch,location,identity,payments,and so on.Similary,in a AI first world,we are rethinking our products and appling machine learning and AI to solve user problems.And we doing this across every one of our products.So today if you using google seach,we rank different using machine learning.Or if you using goole maps,Street View automatically recognizes restaurant signs,street signs,using machine learning.Duo with video calling user machine learning for low bandwidth situations.And Smart Reply and Allo last year had great reception.And so today,we excited that we are rolling out Smart Reply to over one billion users of Gmail.It works very well.Hers's a sample email.if you get an email like this,the machine learning systems learn to be conversational,and it can reply.I'm fine with Saturday,or whatever.So it's really nice to see.Just like every plaform shift,how users interact with computing changes.Mobile brought multi-touch.Similary,we now voice and vision as two new important modalities for computing.Human are interact with computing more inmore natural and immersive ways.Let's start with voice.We've been using voice as an input across many of product our products.That's beacause computer are getting much better at understanding speech.We have had significant breakthroughs,but the pace,even since last year,has been pretty amazing to see.Our word error rate continues to improve,even in very noisy environments.This is why you speak to Google on your phone  or Google Home,we can pick up your voice accurately,even in noisy environments.When we are shipping Google Home,we had originaly planed to include eight microphones so that we could accurately locate the soucre of where the user is speaking from.But thanks to Deep learing,we use a technique called neural beamforming.We were able to ship it with just two microphones and achieve the same quality.Deep learing is what allow us about two weeks ago to announce support for multiple users in Google Home,so that we can recognize up to six people in your house,and personalize experience for each and everyone.So voices is becoming an important modality in our products.The same thing is happening with vision.Simlar to speech,we are seeing great improvements in computer vision.So when we look a picture like this,we are able to understand the attribute behind the picture.We realize is a boy in a birthdat party. There as cake and family involed,and your boy was happy.So we can understand all that better now.And our computer vision system now,for the task of images recofnition,are even better than humans.So it's astounding progress and we are using it across our products.So if you used the Google Pixel,it has the best-of-class camera,and we do a lot of work with computer vision.You can take a low light picture like this,whick is noisy,and we automatically make it much clearer for you.Or coming very soon,if you take a photo of your daughter,at a baseball game,and there is something obstucting it,we can do the hardwork to remove the obstruction and have the picture of what matters to you in front of you.We are clearly at an inflection point with vison.and so tody,we are announcing a new initiative call Google Lens.Google Len is a set of vision-base computing capabilities that can understand what you're looking at,and help take  action based on that infomation.We'll ship it first into Google Assistant and Photos,and it's come to other products.So how does it works? So for example,if you run into something,and you want to know what it is,say,a flower,you can invoke Google Lens from your Assistant,point your phone at it,that we can tell you what flower is it.It's great for someone like me with allergies.Or if you've ever been at a friend'place and you have crowled under a desk,just get the username and password from a Wifi router,you can point your phone at it.And we can automatically do the hard work for you.Or if you're waking in a street downtown,and you see a set of restaurant across you,you can point your phone,we know where you are beacase we have our knowlegde graph and we knowing what you are looking at.and we can give you the right infomation in a meaningful way.As you can see,we've beginning to understand images and videos.All of Google was builded because we started  understanding text and web pages.So the fact that the compuer can understand images and videos has profound  implication for our core mission.When we start working on search,we wanted to do it at scale.This is why we rethought our computational architecure.We designed the data center from the ground up.And we put a lot of effort in them.Now that we are evolving for this machine learing and AI world.we are rethinking our computationl architecure again.We are building what we think of as AI first data centers.This is why last year we launched the Tensor processing unit.
They are custom hardware for machine learning.They are about 15 to 30 times faster and 30 to 80 times more power than CPU an GPU at that time.We use TPU across all our products,every you do a search,every time you speak to Google.In fact ,TPUs are what power AlhpaGo its  historic match  against Lee.As you know,Machine learing as two components.Training,that is how we build the neural net.Training  is very computationally intensive,and inference is what we do at real time,so that when you show it a picture,we'd recognize whether it's a dog or a cat an so on.last year's TPUs were optimized for inference.Training is very computationally intensive.To give you a sense,each one of our machine tranlation models takes a training of over three billion words for a week on about one hundred GPUs.So we've been working hard,and I'm really excited to announce our next generation of TPUS,Cloud TPUs,which are optimized for both traing and inference.What you see behind me is one of Cloud TPUs board.It has four chips on it,and each board is capable for 180 trillion floating point operation per second.They are designed for our data center,so you can easily stack them.You can put 64 of these into one big supercomputer.We call these TPUs pods,and each pod is capable of 11.5 petaflops.It is an important advance for infrastructure for the AI area.The reason we name it Cloud TPU is beacause we're bringing it through the google cloud platform.So Cloud TPUs are coming to google compute engine as of today.We want the google cloud be the best cloud of machine learning.and so we want to provide our customers with a wide range of hardware,be it GPUs,CPUs,including the great GPU Nvidia annouced last week,and now Cloud TPUs.So this lay the fundation for significant progress.So we are focusing on driving  the shift and applying AI to sovle problems. At Google, we're bring our AI effort together under Google.AI.It's a collections effort and teams across the company focused on bring the benefits of AI for everyone.Goolge AI will focus on three areas,state-of-the-art research,tools,infrastructure like tensorFlow,Cloud TPUs and applied AI.So let me take a little bit about these areas.Talking about research,we're excited about designing better machine learing models,but today it is really time consuming.It's a painstaking effort of a few enginerrs and scientist,mainly machine learing Phds.We want it possible for hundred of thousands of develop to use machine learing.So what better way to do this than getting nural nets to design better nueral nets.We call this approach AutoML,it's learing to learn.So the way it  works is we takc a set of candicate nueral nets. Think of this as little baby neural nets And we actually use a nueral net to iterate through them till we arrive the best nueral net.We use reiforcement learing approach.And it's the result is promising.To do this is computationally hard,but Cloud TPUs put it in the realm of possiblity.We are already aproaching state of the art in standart tasks like,say,for our image recognition.So  whenever i Spend time with the team and think nural net building their own nueral nets,it reminds me of one of my favorite movies,Inception.And I tell them we must go deeper.So we are taking all these Ai advances and applying them to newer and harder problems across a wide range of discipline.one such area is health care.Last year,i spoke our work on diabetic retinapathy.it's a preventable casue of blindness.This year,we publish our paper in the "Journal of american medical Association",and verily is bring our products to the medical community.Another such area is pathology.Pathology is a very complex area.if you take a area like breast canner diagnosis,even amongst highly training pathologists,agreement of some forms of breast cancer can be as low as 48 percent.That's because each pathologistd is reviewing the equivalent of 1,000 10-megapixel images for every case.This is large data problem,but one which machine learning is uniquely equipped to solve.So we build nueral net to detect cancer spreading to adjacent lymph node.It'early days,but our nueral nets show a much higher accuracy,89 percent,compared to previous method of 73%.There are important caveats we do have higer false positive.but already giving this in the hands of pathologist.they can improve diagnosis.In general,i think this is a great aproach for machine learning ,provide tools for people to do what they do better.And we're applying across even basic sciences.Take Biology.We're training the neural nets to improve the accuracy of DNA sequencing.Deep varaint is new tools from google ai that indentified genetic varaint more accurately than state-of-the-art method.Reducing error is in important in applications.We can more accurately indetify whether or not a patient has genetic disease and help them with better diagosis and treatment.We're applying it to chemistry.We're using machine learning to predict the properties of molecules.Today it taks a incredible amount of computer resources to hunt for new molecules.and we think we can actuate timelines by orders of magnitude.This opens up possibility in drug discovery or material sciences.I'm entirely confident one day,AI will invent new molecules that behave in predefined ways.Not everything we do are profound.we are doing some even simple and fun things,like a simple tool which can help people to draw.We call ths autoDraw.Just like today when you type today in Google,will give you some suggestion,we can do the same when you're trying to draw.even i can draw with this thing.So it look like fun and games,but push computer to do this thing like this is what help them be creative and actually gain knowledge.So we are very excited about progress even in these area as well.
